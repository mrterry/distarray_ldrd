\documentclass[letterpaper,12pt]{article}
\usepackage{fixltx2e} % LaTeX patches, \textsubscript
\usepackage{cmap} % fix search and cut-and-paste in Acrobat
\usepackage{ifthen}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{longtable,ltcaption,array}
\usepackage{fullpage}
\usepackage{multicol}
\newlength{\DUtablewidth} % internal use in tables

%%% Custom LaTeX preamble
% PDF Standard Fonts
\usepackage{mathptmx} % Times
\usepackage[scaled=.90]{helvet}
\usepackage{courier}

%%% User specified packages and stylesheets

%%% Fallback definitions for Docutils-specific commands

\usepackage[colorlinks=true,linkcolor=blue,urlcolor=blue]{hyperref}
\urlstyle{same} % normal text font (alternatives: tt, rm, sf)

%\title{
%\author{}

\begin{document}

\begin{center}
	\LARGE
	\color{red}
	Interactive Parallel Post-Processing of \\ Hydrocode Output
\end{center}

\vspace{0.5in}

\setlength{\parindent}{0pt}
\large
Laboratory Directed Research and Development \\
Lawrence Livermore National Laboratory \\
Law Wide - New proposal

\normalsize
\begin{multicols}{2}
\textbf{Principle Investigator:}
\columnbreak

Matthew R. Terry \\
Physicist, Fusion Energy Program \\
Heavy Ion Fusion Virtual National Laboratory \\
Physicist and Life Sciences Division

\end{multicols}


\textbf{Co-Investigators:}
\begin{multicols}{2}
	David Grote \\
	Fusion Energy Program \\
	Physics and Life Sciences Division\\
	\columnbreak
			
	Anthony Scopatz \\
	Flash Center \\
	University of Chicago \\
\end{multicols}

%\textbf{Collaborators:}
%\begin{multicols}{2}
%    Joe Koning \\
%    HYDRA \\
%    Engineering Division \\

%    Cyrus Harrison \\
%    VisIt, Somewhere \\
%    Compuational Division(?) \\
%    \columnbreak

%    Dag Sverre Seljebotn \\
%    University of Oslo \\

%    Travis Oliphant (?) \\
%    Continuum Analytics \\
%\end{multicols}


%___________________________________________________________________________

\setlength{\parindent}{15pt}
\section*{Executive Summary }
We propose the development of an interactive computing platform capable of processing memory distributed arrays.  The advent of parallel computing devices has led to dramatic improvements in the scope and fidelity of computational science.  However, these advances have largely left interactive computing behind.  There are specialized tools for visualizing large datasets (such as VisIt\cite{VisIt}), but these do not solve the problem of needing to directly manipulate large datasets \emph{interactively}.  The inability to interactively manipulate large datasets restricts both the scope and the pace of research, particularly at LLNL where large scale simulation is common.  Therefore, we propose the development of a platform with the following capabilities:

\begin{enumerate}
	\item A distributed n-dimensional array, capable of loading too-large-for-serial meshes and having a high level interface that hides the details of how the array is physically distributed.

	\item A comfortable interactive shell, familiar to Yorick or Mathematica users.

	\item Data sharing between the interpreted platform and high quality visualization tool VisIt.
\end{enumerate}

The critical areas of research are identifying exactly how general the array distribution can be while maintaining acceptable performances and identifying how many details of the array distribution to require the user to specify.  The initial application of this platform will target processing of output from LLNL's HYDRA\cite{Marinak2001} radiation-hydrodynamics code, but nearly all of the infrastructure will be directly useful for any structured mesh (HYDRA), AMR (Warp\cite{Grote2005}, FLASH\cite{flash}, ALE-AMR\cite{Koniges2010}), and unstructured mesh codes.  The platform builds on the vibrant Python scientific computing ecosystem, particularly the Numpy\cite{Oliphant2006}, Scipy\cite{numpyscipy}, IPython\cite{ipython} and VisIt projects.

%___________________________________________________________________________

\section*{Technical Description}

The bedrock data structure in scientific computing is the n-dimensional array (nd-array).  It is a memory-contiguous sequence of data of uniform type.  It has the convenient property of resembling both a linear algebra vector/matrix/tensor as well as a being computationally performant data structure.  Due to its ubiquity, there are many high level programming languages that have language-level support (MatLab\cite{matlab}, Yorick\cite{Munro1995}), or widely used libraries (Python\cite{CPython} via Numpy\cite{Oliphant2006}) for manipulating nd-arrays.  One could make a very good argument that the long term popular of Fortran for scientific computing is due to its excellent support for nd-arrays.  These languages are useful across many scientific domains because they leverage the mutual abstraction of the nd-array.

Programs written for distributed memory computers still leverage the nd-array, but scatter the array memory across many machines (``distributed array'').  This allows for arrays with memory requirements larger than the capacity of a single machine and divides the computational work among many processors.  While the distributed array data structure is very common, there is not a standard implementation or direct language support.  The lack of a standard implementation forces each project to implement their own distributed array which is likely incompatible with other projects implementation.  This inhibits code and tool reuse and slows the evolution of scientific software.

In particular, the non-existence of a standard distributed array means that interpret languages have not wrapped it and thus processing large datasets is largely confined to special purpose programs.

We are proposing the development of a distributed array for Python that follows Numpy's tradition of interoperability and adaptability.  Like Numpy's \texttt{ndarray}, our distributed array will be able wrap pre-existing data structures and provide an alternate interface without copying or substantially modifying the original program.  The distributed array will work with HDF5\cite{HDF5} and SILO data files.  Likewise, it will be able to allocate and manage its own memory.  This flexibility requires a flexible representation of the how the array is distributed in memory.  Particularly, it should support the two most common distribution schemes: block structured (used for domain decomposition) and block cyclic (used in parallel linear algebra libraries such as ScaLAPACK\cite{scalapack}).  Distributed arrays of differing distributions must be able to interact and there should be functionality for converting between different distribution schemes.  It is reasonable to expect the performance to vary with distribution schemes, but it will be an area of research to establish exactly how general a distributed array we can represent while maintaining acceptable performance.
%  General purpose algorthms, even with modest performances, can be very useful.  As Knuth is famously quoted, ``Premature optimization is the root of all evil.''\cite{Knuth1974}

To out knowledge, there are two projects that approach the functionality we desire: petsc4py\cite{petsc4py-web-page} (Python bindings for Petsc\cite{petsc-user-ref}) and GAIN\cite{global-arrays-python} (Python bindings for Global Arrays\cite{global-arrays}).  The main objection to both is that they are opinionated projects and insist that your program use the memory they allocate.  This makes them very difficult to adapt to pre-existing projects.  Additionally, the petsc4py API is very verbose and not appropriate for interactive use.

Having developed a scripting language accessible distributed array, we can begin to address the problem that inspired this proposal: post-processing the output of large simulations is more difficult than it ought.  
  
The first component of this is that our distributed array must have an interface that hides the details of the array distribution and communication.  Additionally, we would like an interactive shell that provides variable name completion, history and the other creature comforts that one expects from an interactive shell.  For this the project will build on the well regarded IPython project.

Finally, this data analysis platform must be capable of visualizing the data it processes.  For this, we will rely on the very capable VisIt project.  VisIt is use across many domains

% common data structures/algorithms should have a shared implementation.  Power in shared abstractions

% interactive computing has looser performance requirements, but don't preclude high performance

% flyweight pattern

% However, existing technologies for loading and manipulating data are serial only.  Most are even more restrictive in that they require the data to be (strided) memory contiguous.  This significantly constrains the size of the simulation that can be considered.  Rather than using a serial scripted language, one can explore her data with a parallel visualisation tool such as VisIt\cite{VisIt} or ParaView\cite{paraview}.  While these tools can graphically represent very large data sources, they do not offer direct access to the data arrays.  Directly manipulating data is extremely important in research.

%___________________________________________________________________________

\section*{Significance and Potential Impact}

% integrate somewhere
Because scripting languages lack a distributed nd-array, users must manually extract the ``interesting'' data from a large dataset.  This extraction manifests itself as a serial script that slowly plows through large numbers of binary files, selecting the important data and copying that to a second, smaller binary file.  The decimated data fits in memory and the scientist can use her familiar tools, but this decimation often takes hours, and must be repeated if the initial selection does not contain the desired information.  One of the goals of this proposal is to drastically reduce the friction preventing the scientist from getting work done.


The significance of this proposal is in its integration of existing ideas into a coherent whole.  Distributed arrays, rich interactive interpreters and parallel visualization are not novel concepts, but getting them to interact as a coherent whole is.

There are three key components of this proposal: a Python wrapped distributed array, integration of this distributed array with IPython, and establishing a mechanism to expose distributed array data to VisIt.  The Numpy n-dimensional array is the bedrock on which the Python scientific computing stands.  It provides a powerful and flexible high level (Python) interface, as well as a C-API that allows it to easily interface with pre-existing code bases (LAPACK, etc).  The distributed array aims to be for parallel datasets, what the Numpy \texttt{ndarray} is for serial datasets.

Finally, augmenting the already powerful VisIt with a mechanism to directly manipulate the data would represent a large jump in visualization capability.

While we are developing these technologies in the context of post-processing HYDRA output, they will applicability to most problems using distributed arrays.

% Anthony/FLASH as proof of wide applicability and seed for broader community

% imagination, genomic datasets, lazy evaluation, fancy high level linear algebra


\section*{Plan of Work, Schedule and Budget}

We propose the following tasks over a two year program:

\begin{enumerate}
	\item Develop a general purpose n-dimensional array and furnish with basic data re-distribution routines.  Establish performance boundaries and array distribution limitations. (Terry, Grote, Scopatz, 0.75 FTE) 

	\item Python Bindings and integration within IPython (Terry, Grote 0.50 FTE)

	\item Integration with HYDRA's output files (Terry xx FTE)

	\item Extensions to VisIt expose our distributed arrays and enable parallel visualization with VisIt. (Terry, Grote xxx FTE)

\end{enumerate}

\begin{itemize}
	\setlength{\itemsep}{0pt}
	\setlength{\parskip}{0pt}
	\setlength{\parsep}{0pt}

	\item Funded effort = 300k
	\item Collaboration (unfunded) =
	\item FY2013 request =  300k
	\item FY2014 request =  300k
\end{itemize}


\section*{Desired Outcome, Leverage and Exit Strategy}

Since its founding, LLNL has been a leader in the development and application of high performance computational systems working on large problems.  This project would extend capability to working \emph{interactively} with such systems.  Studies have shown productivity improvements of at least a factor of two\cite{Prechelt2000} in using interpreted languages over compiled ones.  This project would position LLNL to reap those productivity gains even while simulations datasets outgrow serial processing.

The objective of the project is to develop broadly useful infrastructure for manipulating distributed arrays in Python.  The project will leverage LLNL's extensive experience in parallel computation (Grote, Koning) as well as its prominance in the parallel visualization community (Harrison).  Additionally, the project's external collaborators are leaders in their communities, Scopatz as a lead developer of the Pytables project and Oliphant as the creator of Numpy and Scipy.

We envision the the distributed array component attracting interest from a broad community of potential users and contributors.  This proposal alone has interested parties representing two LLNL developed codes (HYDRA and Warp) and an academically developed astro-physics code (FLASH).  After two years, We expect this project to be sufficiently mature and have sufficient outside interest to turn development and maintenance over to a community developed project in the mold of Numpy and Scipy.

%___________________________________________________________________________


\section*{Research Team}

\setlength{\DUtablewidth}{\linewidth}
\begin{longtable*}[c]
	{|p{0.187\DUtablewidth}|p{0.251\DUtablewidth}|p{0.260\DUtablewidth}|p{0.251\DUtablewidth}|}
	\hline
	\textbf{Team Member} & \textbf{Affiliation} & \textbf{Expertise} & \textbf{Task Area} \\
	\endfirsthead
	\hline

	Matthew Terry \newline
	0.5/0.5 FTE &
	Fusion Energy Program &
	ICF, ICF target design, Python, HYDRA, use cases &
	PI for project direction,
	distributed array, IPython integration, HYDRA interoperability \\
	\hline

	David Grote \newline
	0.5/0.3 FTE &
	Fusion Energy Program &
	Accelerator Physics, Parallel Computing &
	MPI distributed arrays, Warp interoperability \\
	\hline

	Anthony Scopatz \newline
	0.1 FTE (FY2013 Only) &
	Flash Center, \newline
	University of Chicago &
	Pytables (Python HDF5 bindings), FLASH, Nuclear fuel cycle, HEDP &
	distributed array, HDF5 interaction, FLASH interoperability \\
	\hline

	Travis Oliphant \newline
	advising &
	Continuum Analytics &
	Python scientific computing infrastructure &
	Numpy compatibility \\
	\hline

	Cyrus Harrision \newline
	advising &
	Computation, \newline
	VisIt Developer &
	Parallel Visualization &
	Interaction with VisIt/VTK  \\
	\hline

	Joe Koning \newline
	advising &
	Engineering, \newline
	HYDRA Developer & 
	ICF, Electro-magnetics, parallel computing & 
	HYDRA interoperability \\
	\hline

	Dag Sverre Seljebotn \newline
	advising &
	Department of Astorophysics, \newline
	University of Oslo &
	Astrophysics, C-Python bindings &
	Distributed array schemes and redistribution \\
	\hline
\end{longtable*}

\bibliographystyle{plain}
\bibliography{/users/terry10/doc/latex_files/terry}

\end{document}
