\documentclass[a4paper]{article}
% generated by Docutils <http://docutils.sourceforge.net/>
\usepackage{fixltx2e} % LaTeX patches, \textsubscript
\usepackage{cmap} % fix search and cut-and-paste in Acrobat
\usepackage{ifthen}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{longtable,ltcaption,array}
\usepackage{fullpage}
\usepackage{multicol}
\setlength{\extrarowheight}{2pt}
\newlength{\DUtablewidth} % internal use in tables

%%% Custom LaTeX preamble
% PDF Standard Fonts
\usepackage{mathptmx} % Times
\usepackage[scaled=.90]{helvet}
\usepackage{courier}

%%% User specified packages and stylesheets

%%% Fallback definitions for Docutils-specific commands

\usepackage[colorlinks=true,linkcolor=blue,urlcolor=blue]{hyperref}
\urlstyle{same} % normal text font (alternatives: tt, rm, sf)

%\title{Interactive Parallel Post-Processing  of Hydrocode Output }
\title{Dapper:\\Handsome, Interactive, Parallel Processing of Hydrocode Output}
\author{Matthew R. Terry}
\date{}

\begin{document}
\maketitle

\setlength{\parindent}{0pt}
\large
Laboratory Directed Research and Development \\
Lawrence Livermore National Laboratory \\
Law Wide - New proposal

\normalsize
%\setlength{\parindent}{15pt}
\setlength{\parindent}{0pt}

\begin{multicols}{2}
\setlength{\parindent}{0pt}
\textbf{Principle Investigator:}
\columnbreak

Matthew R. Terry \\
Physicist, Fusion Energy Program \\
Heavy Ion Fusion Virtual National Laboratory \\
Phyicist and Life Sciences Division

\end{multicols}


\textbf{Co-Invstigators:}
\begin{multicols}{3}
	\setlength{\parindent}{0pt}
	David Grote \\
	Fusion Energy Program \\
	Physics and Life Sciences Division\\
	\columnbreak
			
	Cyrus Harrison \\
	VisIt, Somewhere \\
	Compuational Division(?) \\
	\columnbreak

	Anthony Scopatz \\
	Flash Center \\
	University of Chicago \\
\end{multicols}

\textbf{Collaborators:}
\begin{multicols}{3}
	\setlength{\parindent}{0pt}
	Joe Koning \\
	HYDRA \\
	Engineering Division \\
	\columnbreak

	Dag Sverre Seljebotn \\
	University of Oslo \\
	\columnbreak

	Travis Oliphant (?) \\
	Continuum Analytics \\
\end{multicols}


%___________________________________________________________________________

\section*{Executive Summary }

The advent of parallel computing devices has led to dramatic improvements in the scope and fidelity of computational science.  However, these advances have largely left interactive computing behind.  Parallel visualization tools (such as VisIt) produce outstanding graphics, but tools for directly manipulating large datasets \emph{interactively} do not exist.  This hinders analysing the output of LLNL's many massively parallel physics codes.

Therefore, we propose the development of an interactive computing platform to meet this need.  The platform will feature a distributed n-dimensional array, capable of loading too-large-for-serial meshes and having a high level interface that hides the details of how the array is physically distributed.  The platform will feature a comfortable interactive shell, familiar to Yorick or Mathematica users.  Finally, the platform well leverage the high quality visualization tool VisIt for graphics.

The platform will initially target data generated from the HYDRA\cite{Marinak2001} radiation-hydrodynamics code, but nearly all of the infrastructure will be directly useful for any structured mesh (Hydra, ), AMR (Warp\cite{Grote2005}, Flash\cite{flash}, ALE-AMR\cite{Koniges2010},), and unstructured mesh () codes.  The plaform builds on the vibrant Python scientific computing ecosystem, particularly the Numpy\cite{Oliphant2006}, Scipy\cite{numpyscipy}, IPython\cite{ipython} and VisIt\cite{VisIt} projects.

%___________________________________________________________________________

\section*{Technical Description}

The bedrock data structure in scientific computing is the n-dimensional array (nd-array).  It is a memory contiguous sequence of data of uniform type.  It has the convenient property of resembling both the a linear algebra vector/matrix/tensor as well as a being computationally performant data structure.  Due to its ubiquity, there are several high level languages that have language support (MatLab\cite{matlab}, Yorick\cite{Munro1995}), or widely used libraries (Python\cite{CPython} via Numpy\cite{Oliphant2006}) for manipulating nd-arrays.  One could make a very good argument that the long term popular of Fortran for scientific computing is due to its excellent support for nd-arrays.  These languages are useful across many scientific domains because they leverage the mutual abstraction of the nd-array.

Programs written for distributed memory computers still leverage the nd-array, but use a distributed version (``distributed array'').  Rather than having a single process be responsible for a single contiguous block of memory on a single machine, distributed arrays divide the array into pieces and locate only a few segments on each machine.  While the distributed array data structure is very common, there is not a standard implementation.  The lack of a standard implementation forces each project to implement their own distributed array which is likely incompatible with other projects implementation.  This inhibits code and tool reuse and slows the development of scientific software.

Following the tradition of Numpy, we are proposing the development of a distributed array for Python.  Like Numpy's \texttt{ndarray}, our distributed arary should ``play well with others.''  Our distributed array should work equally well if it allocates its own memory or wraps and applications pre-existing memory.  This means that our distributed array must have a flexible representation of the distribution scheme.  It is reasonable the performance will vary by distribution schemes, but we believe it to be more important to broadly useful than force users to conform to a particular distribution scheme.

We initially propse to develop this platform in the context of post-processing HYDRA binary output, the techniques and infrastruction will not be specific to HYDRA\@.  The distributed array class would be useful for 

Use Scalapack

Researchers are forced into very inefficient workflows due to the limitations of their tools.  Consider the case of examining the output of a simulation where the mesh does not fit into memory.  The scientist has two options: decimate the data such that it will fit into memory or use a parallel visualization tool


%___________________________________________________________________________

\section*{Significance and Potential Impact}
This could go big!

\section*{Plan of Work, Schedule and Budget}

We propose the following tasks over a two year program:

\begin{enumerate}
	\item Development of a distributed n-dimensional array (Terry, Grote)

	\item Python Bindings and integration within IPython

	\item Extensions to VisIt expose our distributed arrays and enable parallel visualization with Visit.
\end{enumerate}


\section*{Disired Outcome, Leverage and Exit Strategy}
Get wide adoption, community support.

%___________________________________________________________________________

\section*{Summary}

This is the summary


\setlength{\DUtablewidth}{\linewidth}
\begin{longtable*}[c]{|p{0.187\DUtablewidth}|p{0.251\DUtablewidth}|p{0.260\DUtablewidth}|p{0.251\DUtablewidth}|}
	\hline
	\textbf{Team Member} & \textbf{Affiliation} & \textbf{Expertise} & \textbf{Task Area} \\
	\hline
	\endfirsthead
	\hline

	Matthew Terry &
	Fusion Energy Program &
	ICF, ICF target design, Python, HYDRA, use cases &
	PI for project direction, distributed array, IPython integration \\
	\hline

	David Grote &
	Fusion Energy Program &
	Accelerator Physics, Parallel Computing &
	MPI Communication for distributed array \\
	\hline

	Cyrus Harrision &
	Computation, VisIt Developer &
	Parallel Visualization &
	Interaction with VisIt/VTK  \\
	\hline

	Joe Koning &
	Engineering, HYDRA Developer & 
	ICF, Electro-magnetics, parallel computing & 
	Interfacing with HYDRA output files \\
	\hline

	Dag & 
	University of Oslo, Department of Astorophysics? & 
	Astrophysics, C-Python bindings &
	Distributed array schemes and conversion \\
	\hline
\end{longtable*}

\bibliographystyle{plain}
\bibliography{/users/terry10/doc/latex_files/terry}

\end{document}
