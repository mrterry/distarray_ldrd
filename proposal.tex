\documentclass[a4paper]{article}
% generated by Docutils <http://docutils.sourceforge.net/>
\usepackage{fixltx2e} % LaTeX patches, \textsubscript
\usepackage{cmap} % fix search and cut-and-paste in Acrobat
\usepackage{ifthen}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{longtable,ltcaption,array}
\usepackage{fullpage}
\usepackage{multicol}
\setlength{\extrarowheight}{2pt}
\newlength{\DUtablewidth} % internal use in tables

%%% Custom LaTeX preamble
% PDF Standard Fonts
\usepackage{mathptmx} % Times
\usepackage[scaled=.90]{helvet}
\usepackage{courier}

%%% User specified packages and stylesheets

%%% Fallback definitions for Docutils-specific commands

\usepackage[colorlinks=true,linkcolor=blue,urlcolor=blue]{hyperref}
\urlstyle{same} % normal text font (alternatives: tt, rm, sf)

%\title{Interactive Parallel Post-Processing  of Hydrocode Output }
\title{Dapper:\\Handsome, Interactive, Parallel Processing of Hydrocode Output}
\author{}
\date{}

\begin{document}
\maketitle

Laboratory Directed Research and Development
Lawrence Livermore National Laboratory
Law Wide - New proposal
%
\begin{multicols}{2}
	\begin{description}
		\item[Principle Investigator:] \leavevmode \\
			Matthew R. Terry, Physicist \\
			Fusion Energy Program \\
			Heavy Ion Fusion Virtual National Laboratory \\
			Physics and Life Sciences

		\item[Co-Invstigators:] \leavevmode \\
			David Grote \\
			Fusion Energy Program \\
			Physics and Life Sciences \\
			
			Cyrus Harrison \\
			VisIt \\

		\columnbreak
		\item[Collaborators:] \leavevmode \\
			Joe Koning \\
			Engineering \\
			HYDRA \\

			Dag Sverre Seljebotn \\
			University of Oslo \\

			Travis Oliphant \\
			Continuum Analytics \\
	\end{description}
\end{multicols}


%___________________________________________________________________________

\section*{Executive Summary }

The advent of parallel computing devices has led to dramatic improvements in the scope and fidelity of computational science.  However, these advances have largely left interactive computing behind.  Parallel visualization tools (such as VisIt) produce outstanding graphics, but tools for directly manipulating large datasets \emph{interactively} do not exist.  This hinders analysing the output of LLNL's many massively parallel physics codes.

Therefore, we propose the development of an interactive computing platform to meet this need.  The platform will feature a distributed n-dimensional array, capable of loading too-large-for-serial meshes and having a high level interface that hides the details of how the array is physically distributed.  The platform will feature a comfortable interactive shell, familiar to Yorick or Mathematica.  Finally, the platform well leverage the high quality visualization tool VisIt for graphics.

The plaform builds on the vibrant Python scientific computing ecosystem, particularly the Numpy\cite{numpysciyp}, Scipy, IPython\cite{ipython} and VisIt\cite{visit} projects.

%___________________________________________________________________________

\section*{Technical Description}

At the present, there are two dominate means of interactively post-processing the output of large simulations.  The first is to process the data using a scripting language like Python or Yorick.  These languages have powerful high level constructs for working with data.  However, existing technologies for loading and manipulating data are serial only.  This significantly constrains the size of the simulation that can be considered.  The second method is to use a parallel visualisation tool like VisIt or Paraview.  While these tools can accept very large data sources, they lack the flexibility of direct access using a scripting language.

We propose the development of a script-able platform for manipulating parallel data with a familiar serial-like interface.  The goal is to build the correct infrastructure such that the user can make use of additional cores and additional machines, but without manually managing parallel data.  The platform would use the Python language and extend the existing Numpy N-dimensional array to work with memory distributed data.  The platform would build on the existing IPython clustering and interactivity work.  The platform would initially be targeted at processing Hydra output, but techniques and software would be easily extend-able to other codes using a structured mesh and possibly to those with unstructured meshes.

We initially propse to develop this platform in the context of post-processing HYDRA binary output, the techniques and infrastruction will not be specific to HYDRA\@.  The distributed array class would be useful for 

Use Scalapack

Researchers are forced into very inefficient workflows due to the limitations of their tools.  Consider the case of examining the output of a simulation where the mesh does not fit into memory.  The scientist has two options: decimate the data such that it will fit into memory or use a parallel visualization tool


%___________________________________________________________________________

\section*{Significance and Potential Impact}
This could go big!

\section*{Plan of Work, Schedule and Budget}

We propose the following tasks over a two year program:

\begin{enumerate}
	\item Development of a distributed n-dimensional array (Terry, Grote)

	\item Python Bindings and integration within IPython

	\item Extensions to VisIt expose our distributed arrays and enable parallel visualization with Visit.
\end{enumerate}


\section*{Disired Outcome, Leverage and Exit Strategy}
Get wide adoption, community support.

%___________________________________________________________________________

\section*{Summary}

This is the summary


\setlength{\DUtablewidth}{\linewidth}
\begin{longtable*}[c]{|p{0.187\DUtablewidth}|p{0.251\DUtablewidth}|p{0.260\DUtablewidth}|p{0.251\DUtablewidth}|}
	\hline
	\textbf{Team Member} & \textbf{Affiliation} & \textbf{Expertise} & \textbf{Task Area} \\
	\hline
	\endfirsthead
	\hline

	Matthew Terry &
	Fusion Energy Program &
	ICF, ICF target design, Python, HYDRA, use cases &
	PI for project direction, distributed array, IPython integration \\
	\hline

	David Grote &
	Fusion Energy Program &
	Accelerator Physics, Parallel Computing &
	MPI Communication for distributed array \\
	\hline

	Cyrus Harrision &
	Computation, VisIt Developer &
	Parallel Visualization &
	Interaction with VisIt/VTK  \\
	\hline

	Joe Koning &
	Engineering, HYDRA Developer & 
	ICF, Electro-magnetics, parallel computing & 
	Interfacing with HYDRA output files \\
	\hline

	Dag & 
	University of Oslo, Department of Astorophysics? & 
	Astrophysics, C-Python bindings &
	Distributed array schemes and conversion \\
	\hline
\end{longtable*}

\bibliography{/users/terry10/doc/latex_files/terry}

\end{document}
