\documentclass[letterpaper,12pt]{article}
\usepackage{fixltx2e} % LaTeX patches, \textsubscript
\usepackage{cmap} % fix search and cut-and-paste in Acrobat
\usepackage{ifthen}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{longtable,ltcaption,array}
\usepackage{fullpage}
\usepackage{multicol}
\newlength{\DUtablewidth} % internal use in tables

%%% Custom LaTeX preamble
% PDF Standard Fonts
\usepackage{mathptmx} % Times
\usepackage[scaled=.90]{helvet}
\usepackage{courier}

%%% User specified packages and stylesheets

%%% Fallback definitions for Docutils-specific commands

\usepackage[colorlinks=true,linkcolor=blue,urlcolor=blue]{hyperref}
\urlstyle{same} % normal text font (alternatives: tt, rm, sf)

%\title{
%\author{}

\begin{document}

\begin{center}
	\LARGE
	\color{red}
	Interactive Parallel Post-Processing of \\
	Advanced Simulation Code Output
\end{center}

\vspace{0.5in}

\setlength{\parindent}{0pt}
\large
Laboratory Directed Research and Development \\
Lawrence Livermore National Laboratory \\
Law Wide - New proposal

\normalsize
\begin{multicols}{2}
\textbf{Principle Investigator:}
\columnbreak

Matthew R. Terry \\
Physicist, Fusion Energy Program \\
Heavy Ion Fusion Virtual National Laboratory \\
Physicist and Life Sciences Division

\end{multicols}


\textbf{Co-Investigators:}
\begin{multicols}{2}
	David Grote \\
	Fusion Energy Program \\
	Physics and Life Sciences Division\\
	\columnbreak
			
	Anthony Scopatz \\
	The FLASH Center \\
	The University of Chicago \\
\end{multicols}

%\textbf{Collaborators:}
%\begin{multicols}{2}
%    Joe Koning \\
%    HYDRA \\
%    Engineering Division \\

%    Cyrus Harrison \\
%    VisIt, Somewhere \\
%    Compuational Division(?) \\
%    \columnbreak

%    Dag Sverre Seljebotn \\
%    University of Oslo \\

%    Travis Oliphant (?) \\
%    Continuum Analytics \\
%\end{multicols}


%___________________________________________________________________________

\setlength{\parindent}{15pt}
\section*{Executive Summary }
We propose the development of an interactive computing platform capable of processing memory distributed arrays.  The advent of parallel computing devices has led to dramatic improvements in the scope and fidelity of computational science.  However, these advances have largely left interactive computing behind.  There are specialized tools for visualizing large datasets (such as VisIt\cite{VisIt}), but these do not solve the problem of needing to directly manipulate large datasets \emph{interactively}.  The inability to interactively manipulate large datasets restricts both the scope and the pace of research, particularly at LLNL where large scale simulation is common.  Therefore, we propose the development of a platform with the following capabilities:

\begin{enumerate}
	\item A distributed n-dimensional array, capable of loading too-large-for-serial meshes 
          and having a high-level interface which automatially handles how 
          the array is physically distributed.

	\item A comfortable interactive shell, familiar to Yorick or Mathematica users.

	\item Data sharing between the interpreted platform and high quality visualization tool VisIt.
\end{enumerate}

The critical areas of research are identifying how general the array distribution can be 
while maintaining acceptable performance and identifying how many details of the array 
distribution to require the user to specify.  The initial application of this platform 
will target processing of output from LLNL's HYDRA\cite{Marinak2001} radiation-hydrodynamics 
code, but nearly all of the infrastructure will be directly useful for any structured mesh 
(HYDRA), AMR (Warp\cite{Grote2005}, FLASH\cite{flash}, ALE-AMR\cite{Koniges2010}), and 
unstructured mesh codes.  The platform builds on the vibrant Python scientific computing 
ecosystem, particularly the NumPy\cite{Oliphant2006}, SciPy\cite{numpyscipy}, 
IPython\cite{ipython} and VisIt projects.

%___________________________________________________________________________

\section*{Technical Description}

The bedrock data structure in scientific computing is the n-dimensional array (nd-array).  
It is a memory-contiguous sequence of data of uniform type.  It has the convenient property 
of resembling both a linear algebra vector/matrix/tensor as well as a being computationally 
performant data structure.  Due to its ubiquity, there are many high level programming 
languages that have language-level support (MatLab\cite{matlab}, Yorick\cite{Munro1995}), 
or widely used libraries (Python\cite{CPython} via NumPy\cite{Oliphant2006}) for 
manipulating nd-arrays.  The long-term popularity of Fortran for scientific computing is 
due in part to its excellent support for nd-arrays.  These languages are useful across 
many scientific domains because they leverage the mutual abstraction of the nd-array.

Programs written for distributed memory computers still leverage the nd-array, but scatter 
the array memory across many machines (``distributed array'').  This allows for arrays 
with memory requirements larger than the capacity of a single machine \emph{and} divides 
the computational work among many processors.  While the distributed array data structure 
is very common, there is neither a standard implementation nor direct language support.  
The lack of a standard abstraction forces each project to write their own data structure 
which are often incompatible with other implementations.  This inhibits code and tool 
reuse and slows the evolution of scientific software.

In particular, the lack of a standard distributed array means that very high-level 
languages have not wrapped such an object in a dynamic interface.  Thus, processing large 
datasets is largely confined to special purpose software which is not available to all 
computational scientists.

We propose the development of a distributed array package for Python that follows NumPy's 
tradition of interoperability and adaptability.  Like Numpy's \texttt{ndarray}, this 
distributed array will be able wrap pre-existing distributed data structures and provide 
an alternate interface without copying or substantially modifying the original program.  
The distributed array will work with HDF5\cite{HDF5} and SILO data files.  Likewise, it 
will be able to allocate and manage its own memory.  This flexibility requires a dynamic 
representation of the how the array is distributed across the memory resources available.  
Particularly, it should support the two most common distribution schemes: block structured 
(used for domain decomposition) and block cyclic (used in parallel linear algebra libraries 
such as ScaLAPACK\cite{scalapack}).  Distributed arrays of differing distributions must be 
able to interact and there should be functionality for converting between different 
distribution schemes.  It is reasonable to expect the performance to vary with distribution 
schemes.  It will be an area of research to establish the generality of distribution 
schemas we may support while maintaining acceptable performance.
%  General purpose algorthms, even with modest performances, can be very useful.  As Knuth is famously quoted, ``Premature optimization is the root of all evil.''\cite{Knuth1974}

To date, there are two projects that approach in part the functionality we desire: 
petsc4py\cite{petsc4py-web-page} (Python bindings for Petsc\cite{petsc-user-ref}) 
and GAIN\cite{global-arrays-python} (Python bindings for Global Arrays\cite{global-arrays}).  
The main objection to both is that they each and insist that the program use the 
memory which they allocate.  This provides a large barrier to entry for pre-existing 
projects.  Additionally, the petsc4py API is deemed too verbose for interactive use.

Having developed a distributed array which is accessible from both low and high-level 
languages we can address the processing of large data sets.  In particular, we seek 
ease the burden of post-processing the output of large simulations. 
The first component of this is that the distributed array must have an interface that 
automatically manages the details of the array distribution and communication.  
Additionally, we will provide an interactive shell that has variable name completion, 
history, and other user's-time saving features.  For such a shell, this project will 
rely heavily the IPython project, whose backend kernel was recently rewritten to be a 
sophisticated many-to-many parallel compute architecture.  Finally, this data analysis 
platform must be capable of visualization.  Here, we will integrate with LLNL's own VisIt 
project \cite{needed}.  VisIt is used across many domains for dynamic, distributed, 
high-performance visualization.

% common data structures/algorithms should have a shared implementation.  Power in shared abstractions

% interactive computing has looser performance requirements, but don't preclude high performance

% flyweight pattern

% However, existing technologies for loading and manipulating data are serial only.  Most are even more restrictive in that they require the data to be (strided) memory contiguous.  This significantly constrains the size of the simulation that can be considered.  Rather than using a serial scripted language, one can explore her data with a parallel visualisation tool such as VisIt\cite{VisIt} or ParaView\cite{paraview}.  While these tools can graphically represent very large data sources, they do not offer direct access to the data arrays.  Directly manipulating data is extremely important in research.

%___________________________________________________________________________

\section*{Significance and Potential Impact}

Because high-level languages lack a distributed nd-array, users must manually extract a subset
 of ``interesting'' data from a larger dataset.  This extraction process typically manifests 
as a serial script that slowly plows through large numbers of binary files, selects the 
important data out, and copies it to a second, smaller file.  The final decimated data fits 
in memory and the scientist may then use familiar tools.   However this decimation often takes 
hours (or days) and must be repeated if the initial selection does not contain the desired 
information.  Such repreated decimation is a gross waste of time and materials when considering 
the large number of scientisists who currently employ this data management design pattern and
the lack of new fudemental knowledged gained from the practice.

The significance of this proposal, therefore, is that it would remove much of the friction 
associated with working with large datasets.  Though the individual components of the 
proposal - distributed arrays, interactive interpreters, and parallel visualization - are 
not novel concepts, coherently integrating them is.  This proposal represents a large 
advance in the capabilities available to all scientists who must work with large data, 
independent of their individual computational abilites.

There are three main thrusts of this proposal: a Python-wrapped distributed array, integration 
of this distributed array with parallel compute architechture of IPython, and fully exposing 
the distributed array data to VisIt.  

The NumPy n-dimensional array is the bedrock on which the Python scientific computing stands.  
It provides a powerful and flexible high level (Python) interface, as well as a C-API that 
allows it to easily interface with pre-existing code bases (LAPACK, etc).  The distributed 
array aims to be for parallel datasets what the Numpy \texttt{ndarray} is for serial datasets.

Moreover, augmenting the already powerful VisIt with a mechanism to directly manipulate the 
data would represent a large jump in visualization capability.  VisIt users would be able 
to invent quantities to visualize without the intermediate step of putting it in a binary 
file or relying on VisIt to anticipate a particular transformation.

While we are developing these technologies in the context of post-processing HYDRA output, 
they will apply to most problems where a distributed array is the correct abstraction.  
For example, this platform would be very easily adapted for use with the laboratory's large 
design codes.

% imagination, genomic datasets, lazy evaluation, fancy high level linear algebra


\section*{Plan of Work, Schedule and Budget}

We propose the following tasks over a two year program:

\begin{enumerate}
	\item Develop a general purpose n-dimensional array and furnish with basic data re-distribution routines.  Establish performance boundaries and array distribution limitations. (Terry, Grote, Scopatz, 0.75 FTE) 

	\item Python Bindings and integration within IPython (Terry, Grote 0.50 FTE)

	\item Integration with HYDRA's output files (Terry xx FTE)

	\item Extensions to VisIt expose our distributed arrays and enable parallel visualization with VisIt. (Terry, Grote xxx FTE)

\end{enumerate}

\begin{itemize}
	\setlength{\itemsep}{0pt}
	\setlength{\parskip}{0pt}
	\setlength{\parsep}{0pt}

	\item Funded effort = 300k
	\item Collaboration (unfunded) =
	\item FY2013 request =  300k
	\item FY2014 request =  300k
\end{itemize}


\section*{Desired Outcome, Leverage and Exit Strategy}

Since its founding, LLNL has been a leader in the development and application of high 
performance computational systems working on large problems.  This project would extend 
capability to working \emph{interactively} with such systems.  Studies have shown productivity 
improvements of at least a factor of two\cite{Prechelt2000} in using interpreted languages 
over compiled ones.  This project would position LLNL to reap those productivity gains even 
while simulations datasets outgrow serial processing.

The objective of the project is to develop broadly useful infrastructure for manipulating 
distributed arrays in Python.  The project will leverage LLNL's extensive experience in 
parallel computation (Grote, Koning) as well as its prominance in the parallel visualization 
community (Harrison).  Additionally, the project's external collaborators are leaders in their 
communities, Scopatz as a lead developer of the PyTables project and Oliphant as the creator of 
NumPy and SciPy.

We envision the distributed array component attracting interest from a broad community 
of potential users and contributors.  This proposal alone has interested parties 
representing two LLNL developed codes (HYDRA and Warp) and an academically developed 
astrophysics \& high-energy density physics code (FLASH).  After two years, we expect this 
project to be sufficiently mature and have sufficient outside interest to turn development 
and maintenance over to a community governance team in the model of NumPy, Scipy, and IPython.

%___________________________________________________________________________


\section*{Research Team}

\setlength{\DUtablewidth}{\linewidth}
\begin{longtable*}[c]
	{|p{0.187\DUtablewidth}|p{0.251\DUtablewidth}|p{0.260\DUtablewidth}|p{0.251\DUtablewidth}|}
	\hline
	\textbf{Team Member} & \textbf{Affiliation} & \textbf{Expertise} & \textbf{Task Area} \\
	\endfirsthead
	\hline

	Matthew Terry \newline
	0.5/0.5 FTE &
	Fusion Energy Program &
	ICF, ICF target design, Python, HYDRA, use cases &
	PI for project direction,
	distributed array, IPython integration, HYDRA interoperability \\
	\hline

	David Grote \newline
	0.5/0.3 FTE &
	Fusion Energy Program &
	Accelerator Physics, Parallel Computing &
	MPI distributed arrays, Warp interoperability \\
	\hline

	Anthony Scopatz \newline
	0.1 FTE (FY2013 Only) &
	Flash Center, \newline
	University of Chicago &
	Pytables (Python HDF5 bindings), FLASH, Nuclear fuel cycle, HEDP &
	distributed array, HDF5 interaction, FLASH interoperability \\
	\hline

	Travis Oliphant \newline
	advising &
	Continuum Analytics &
	Python scientific computing infrastructure &
	NumPy compatibility \\
	\hline

	Cyrus Harrision \newline
	advising &
	Computation, \newline
	VisIt Developer &
	Parallel Visualization &
	Interaction with VisIt/VTK  \\
	\hline

	Joe Koning \newline
	advising &
	Engineering, \newline
	HYDRA Developer & 
	ICF, Electro-magnetics, parallel computing & 
	HYDRA interoperability \\
	\hline

	Dag Sverre Seljebotn \newline
	advising &
	Department of Astorophysics, \newline
	University of Oslo &
	Astrophysics, C-Python bindings &
	Distributed array schemes and redistribution \\
	\hline
\end{longtable*}

\bibliographystyle{plain}
\bibliography{/users/terry10/doc/latex_files/terry}

\end{document}
