\documentclass[a4paper]{article}
% generated by Docutils <http://docutils.sourceforge.net/>
\usepackage{fixltx2e} % LaTeX patches, \textsubscript
\usepackage{cmap} % fix search and cut-and-paste in Acrobat
\usepackage{ifthen}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{longtable,ltcaption,array}
\usepackage{fullpage}
\usepackage{multicol}
\newlength{\DUtablewidth} % internal use in tables

%%% Custom LaTeX preamble
% PDF Standard Fonts
\usepackage{mathptmx} % Times
\usepackage[scaled=.90]{helvet}
\usepackage{courier}

%%% User specified packages and stylesheets

%%% Fallback definitions for Docutils-specific commands

\usepackage[colorlinks=true,linkcolor=blue,urlcolor=blue]{hyperref}
\urlstyle{same} % normal text font (alternatives: tt, rm, sf)

\title{Interactive Parallel Post-Processing of Hydrocode Output }
\author{}
\date{}

\begin{document}
\maketitle

\setlength{\parindent}{0pt}
\large
Laboratory Directed Research and Development \\
Lawrence Livermore National Laboratory \\
Law Wide - New proposal

\normalsize 
\begin{multicols}{2}
\textbf{Principle Investigator:}
\columnbreak

Matthew R. Terry \\
Physicist, Fusion Energy Program \\
Heavy Ion Fusion Virtual National Laboratory \\
Phyicist and Life Sciences Division

\end{multicols}


\textbf{Co-Invstigators:}
\begin{multicols}{3}
	David Grote \\
	Fusion Energy Program \\
	Physics and Life Sciences Division\\
	\columnbreak
			
	Cyrus Harrison \\
	VisIt, Somewhere \\
	Compuational Division(?) \\
	\columnbreak

	Anthony Scopatz \\
	Flash Center \\
	University of Chicago \\
\end{multicols}

\textbf{Collaborators:}
\begin{multicols}{3}
	Joe Koning \\
	HYDRA \\
	Engineering Division \\
	\columnbreak

	Dag Sverre Seljebotn \\
	University of Oslo \\
	\columnbreak

	Travis Oliphant (?) \\
	Continuum Analytics \\
\end{multicols}


%___________________________________________________________________________

\setlength{\parindent}{15pt}
\section*{Executive Summary }

The advent of parallel computing devices has led to dramatic improvements in the scope and fidelity of computational science.  However, these advances have largely left interactive computing behind.  Parallel visualization tools (such as VisIt) produce outstanding graphics, but tools for directly manipulating large datasets \emph{interactively} do not exist.  Because researchers lack the appropriate interactive tools, they are forced into very inefficient workflows.  Scientists often have special purpose scripts to carve out sections of their simulation data that will fit on a single machine so that they can use the powerful scripting tools they are used to.  These scripts can take hours to step through large amounts of data and amount to a large loss of productivity and hinders the analysing the output of LLNL's many massively parallel physics codes.

Therefore, we propose the development of an interactive computing platform to meet this need.  The platform will feature a distributed n-dimensional array, capable of loading too-large-for-serial meshes and having a high level interface that hides the details of how the array is physically distributed.  The platform will feature a comfortable interactive shell, familiar to Yorick or Mathematica users.  Finally, the platform well leverage the high quality visualization tool VisIt for graphics.

The platform will initially target data generated from the HYDRA\cite{Marinak2001} radiation-hydrodynamics code, but nearly all of the infrastructure will be directly useful for any structured mesh (Hydra, ), AMR (Warp\cite{Grote2005}, Flash\cite{flash}, ALE-AMR\cite{Koniges2010},), and unstructured mesh () codes.  The plaform builds on the vibrant Python scientific computing ecosystem, particularly the Numpy\cite{Oliphant2006}, Scipy\cite{numpyscipy}, IPython\cite{ipython} and VisIt\cite{VisIt} projects.

%___________________________________________________________________________

\section*{Technical Description}

The bedrock data structure in scientific computing is the n-dimensional array (nd-array).  It is a memory contiguous sequence of data of uniform type.  It has the convenient property of resembling both the a linear algebra vector/matrix/tensor as well as a being computationally performant data structure.  Due to its ubiquity, there are several high level languages that have language support (MatLab\cite{matlab}, Yorick\cite{Munro1995}), or widely used libraries (Python\cite{CPython} via Numpy\cite{Oliphant2006}) for manipulating nd-arrays.  One could make a very good argument that the long term popular of Fortran for scientific computing is due to its excellent support for nd-arrays.  These languages are useful across many scientific domains because they leverage the mutual abstraction of the nd-array.

Programs written for distributed memory computers still leverage the nd-array, but scatter the array memory across many machines (``distributed array'').  This allows for arrays with memory requirements larger than the capacity of a single machine and divides the computational work among many processors.  Rather than having a single process be responsible for a single contiguous block of memory on a single machine, distributed arrays divide the array into pieces and locate only a few segments on each machine.  While the distributed array data structure is very common, there is not a standard implementation.  The lack of a standard implementation forces each project to implement their own distributed array which is likely incompatible with other projects implementation.  This inhibits code and tool reuse and slows the development of scientific software.

We are proposing the development of a distributed array for Python that follows Numpy's tradition of adaptability.  Like Numpy's \texttt{ndarray}, our distributed arary should work equally well if it operates on memory it allocates or wraps an application's pre-existing memory.  This means that our distributed array must have a flexible representation of the distribution scheme.  Particularly, it should support the two most common distribution schems: block structured (used for domain decomposition) and block cyclic (used in parallel linear algebra libraries such as ScaLAPACK\cite{scalapack}).  Distributed arrays of differeing distributions must be able to interact and there should be funcionality for converting between different distribution schemes.  It is reasonable to expect the performance to vary with distribution schemes, but we believe it to be more important to broadly useful than force users to conform to a particular distribution scheme.  General purpose algorthms, even with modest performances, can be very useful.  As Knuth is famously quoted, ``Premature optimization is the root of all evil.''\cite{Knuth1974}

Having developed a scripting language accessible distributed array, we can begin to address the problem that inspired this proposal: post-processing the output of large simulations is more difficult than it ought.  The first component of this is that our distributed array must have an interface that hides the details of the array distribution and communication.  Additionally, we would like the an interactive shell that provides variable name completion, history and the other creature comforts that one expects from an interactive shell.  For this the project will use the well regarded IPython.  IPython's new notebook\cite{ipython-notebook}.

% common data structures/algorithms should have a shared implementation.  Power in shared abstractions

% Applications drive the infrastructure.

% interactive computing has looser performance requirements, but don't preclude high performance

% existing work: Global Arrays, Petc and Trillinos


%___________________________________________________________________________

\section*{Significance and Potential Impact}
The significance of this proposal is in its integration of existing ideas into a coherent whole.  Distributed arrays, rich interactive interpreters and parallel visualization are not novel concepts, but getting them to interact as a coherent whole is.

There are three key components of this proposal: a Python wrapped distributed array, integration of this distributed array with IPython, and establishing a mechanism to expose distributed array data to VisIt.  The Numpy n-dimensional array is the bedrock on which the Python scientific computing stands.  It provides a powerful and flexible high level (Python) interface, as well as a C-API that allows it to easily interface with pre-existing code bases (LAPACK, etc).  The distributed array aims to be for parallel datasets, what the Numpy \texttt{ndarray} is for serial datasets.

Finally, augmenting the already powerful VisIt with a mechanism to directly manipulate the data would represent a large jump in visualization capability.

While we are developing these technologies in the context of post-processing Hydra output, they will applicability to most problems using distributed arrays.

% Anthony/Flash as proof of wide applicability and seed for broader community


\section*{Plan of Work, Schedule and Budget}

We propose the following tasks over a two year program:

\begin{enumerate}
	\item Development of a distributed n-dimensional array (Terry, Grote, Scopatz)

	\item Python Bindings and integration within IPython

	\item Integration with HYDRA's output files

	\item Extensions to VisIt expose our distributed arrays and enable parallel visualization with VisIt.
\end{enumerate}


\section*{Desired Outcome, Leverage and Exit Strategy}

The objective of the project is to develop broadly useful infrastructure for manipulating distributed arrays in Python.  The project will leverage LLNL's extensive experience in parallel computation (Grote, Koning) as well as its prominance in the parallel visualization community (Harrison).   The distributed array component of the project should transition to a community developed and maintained software project in the mold of Numpy.

Get wide adoption, community support.

%With the basis of a shared implemention of a distributed array, we can start building a community around its usage.  In addition to the distributed array work, we propose two such extensions: interactive usage and interaction with VisIt.  Potential 

%Our initial application of this distributed array will be for post-processing the binary output of Hydra.  Indeed, this is not the sole application of the project.  Collaborators have interest applications for Warp and for Flash, but 
%___________________________________________________________________________

\section*{Summary}

This is the summary


\setlength{\DUtablewidth}{\linewidth}
\begin{longtable*}[c]{|p{0.187\DUtablewidth}|p{0.251\DUtablewidth}|p{0.260\DUtablewidth}|p{0.251\DUtablewidth}|}
	\hline
	\textbf{Team Member} & \textbf{Affiliation} & \textbf{Expertise} & \textbf{Task Area} \\
	\hline
	\endfirsthead
	\hline

	Matthew Terry &
	Fusion Energy Program &
	ICF, ICF target design, Python, HYDRA, use cases &
	PI for project direction, distributed array, IPython integration, Hydra interoperability \\
	\hline

	David Grote &
	Fusion Energy Program &
	Accelerator Physics, Parallel Computing &
	MPI distributed arrays, Warp interoperability \\
	\hline

	Anthony Scopatz &
	FLASH Center, University of Chicago &
	Scientific computing in Python, AMR, Nuclear fuel cycle, HEDP &
	distributed array, Flash interoperability \\
	\hline

	Cyrus Harrision &
	Computation, VisIt Developer &
	Parallel Visualization &
	Interaction with VisIt/VTK  \\
	\hline

	Joe Koning &
	Engineering, HYDRA Developer & 
	ICF, Electro-magnetics, parallel computing & 
	HYDRA interoperability \\
	\hline

	Dag Sverre Seljebotn &
	University of Oslo, Department of Astorophysics? & 
	Astrophysics, C-Python bindings &
	Distributed array schemes and redistribution \\
	\hline

	Travis Oliphant &
	Continuum Analytics &
	Python scientific computing infrastructure &
	Numpy compatibility \\

	\hline
\end{longtable*}

\bibliographystyle{plain}
\bibliography{/users/terry10/doc/latex_files/terry}

\end{document}
